{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sebastián Rodríguez\n",
    "## 20003076\n",
    "\n",
    "# Laboratorio # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionar las imágenes que tenemos\n",
    "img_width, img_height = 150, 150\n",
    "# Directorios para encontrar los dataset suministrados\n",
    "Entrena_Dir = 'data/train'\n",
    "Val_Dir = 'data/validation'\n",
    "\n",
    "# Cantidad de imagenes utilizadas para entrenamiento y validaciones\n",
    "# Se coloca una cantidad de epochs en 50 para obtener el coeficiente de 0.8 buscado\n",
    "Pruebas_ent = 2000\n",
    "Preubas_Val = 800\n",
    "epochs = 50\n",
    "batchsize = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red convolucional\n",
    "\n",
    "Se utilizó  una red neuronal convolucional pequeña con pocas capas y pocos filtros por capa, adicional se utilizó aumento y la eliminación de datos. \n",
    "\n",
    "Abajo es el primer modelo, una simple pila de 3 capas de convolución con una activación ReLU y seguida por capas de máxima acumulación. Algo que llama la atención es el data augmentation puede perturbar las correlaciones aleatorias que pueden estar presentes en el data set sin saberlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA AUGMENTATION Y PRE-PROCESSING\n",
    "\n",
    "Vamos a utilizar ImageDataGenerator para aumentar la cantidad de información que podemos obtener de nuestro set de entrenamiento. Vamos a realizar transformaciones en las imágenes que nos ayudará a que el modelo generalice mejor y evitemos overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.6199 - accuracy: 0.6690 - val_loss: 0.5621 - val_accuracy: 0.7050\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 29s 228ms/step - loss: 0.6110 - accuracy: 0.6765 - val_loss: 0.5720 - val_accuracy: 0.6913\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 30s 239ms/step - loss: 0.6076 - accuracy: 0.6730 - val_loss: 0.5649 - val_accuracy: 0.7237\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.5976 - accuracy: 0.6815 - val_loss: 0.7197 - val_accuracy: 0.6750\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.5997 - accuracy: 0.6795 - val_loss: 0.5034 - val_accuracy: 0.7175\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.5739 - accuracy: 0.7050 - val_loss: 0.6250 - val_accuracy: 0.7125\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.5813 - accuracy: 0.7025 - val_loss: 0.4883 - val_accuracy: 0.7475\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.5683 - accuracy: 0.7055 - val_loss: 0.4926 - val_accuracy: 0.7337\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.5626 - accuracy: 0.7345 - val_loss: 0.2438 - val_accuracy: 0.7575\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.5509 - accuracy: 0.7420 - val_loss: 0.8084 - val_accuracy: 0.7262\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.5781 - accuracy: 0.7215 - val_loss: 0.3206 - val_accuracy: 0.7725\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.5432 - accuracy: 0.7360 - val_loss: 0.3824 - val_accuracy: 0.7500\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.5246 - accuracy: 0.7515 - val_loss: 0.9212 - val_accuracy: 0.7375\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.5454 - accuracy: 0.7420 - val_loss: 0.6557 - val_accuracy: 0.7225\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.5351 - accuracy: 0.7430 - val_loss: 0.8780 - val_accuracy: 0.7663\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.5237 - accuracy: 0.7565 - val_loss: 0.4944 - val_accuracy: 0.7575\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 29s 228ms/step - loss: 0.5261 - accuracy: 0.7535 - val_loss: 0.3982 - val_accuracy: 0.7837\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.5121 - accuracy: 0.7650 - val_loss: 0.6503 - val_accuracy: 0.7750\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.5108 - accuracy: 0.7740 - val_loss: 0.2961 - val_accuracy: 0.7713\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.5088 - accuracy: 0.7520 - val_loss: 0.3190 - val_accuracy: 0.6637\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.5234 - accuracy: 0.7545 - val_loss: 0.3572 - val_accuracy: 0.7887\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.5242 - accuracy: 0.7610 - val_loss: 0.5586 - val_accuracy: 0.7887\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.4955 - accuracy: 0.7725 - val_loss: 0.4898 - val_accuracy: 0.7812\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.4980 - accuracy: 0.7780 - val_loss: 0.4696 - val_accuracy: 0.7688\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 29s 235ms/step - loss: 0.4851 - accuracy: 0.7790 - val_loss: 0.5633 - val_accuracy: 0.7850\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.4785 - accuracy: 0.7855 - val_loss: 0.9824 - val_accuracy: 0.7763\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.4959 - accuracy: 0.7665 - val_loss: 0.7801 - val_accuracy: 0.7825\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 29s 229ms/step - loss: 0.5005 - accuracy: 0.7685 - val_loss: 0.2767 - val_accuracy: 0.7725\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.4779 - accuracy: 0.7665 - val_loss: 0.2313 - val_accuracy: 0.7525\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.4891 - accuracy: 0.7855 - val_loss: 0.7433 - val_accuracy: 0.7825\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.5027 - accuracy: 0.7670 - val_loss: 0.3882 - val_accuracy: 0.7837\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 38s 308ms/step - loss: 0.4887 - accuracy: 0.7780 - val_loss: 0.3334 - val_accuracy: 0.8062\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 38s 307ms/step - loss: 0.4685 - accuracy: 0.7995 - val_loss: 0.4764 - val_accuracy: 0.8200\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 38s 307ms/step - loss: 0.4991 - accuracy: 0.7660 - val_loss: 0.5829 - val_accuracy: 0.7588\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 39s 316ms/step - loss: 0.4924 - accuracy: 0.7690 - val_loss: 0.4176 - val_accuracy: 0.7850\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.4702 - accuracy: 0.7900 - val_loss: 0.4609 - val_accuracy: 0.7912\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 39s 313ms/step - loss: 0.4880 - accuracy: 0.7770 - val_loss: 0.6969 - val_accuracy: 0.7887\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 39s 314ms/step - loss: 0.4481 - accuracy: 0.8045 - val_loss: 0.3149 - val_accuracy: 0.8300\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 39s 311ms/step - loss: 0.4855 - accuracy: 0.7825 - val_loss: 0.3562 - val_accuracy: 0.8288\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 39s 312ms/step - loss: 0.4702 - accuracy: 0.7930 - val_loss: 0.4564 - val_accuracy: 0.7800\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 40s 316ms/step - loss: 0.4831 - accuracy: 0.7860 - val_loss: 0.3703 - val_accuracy: 0.7850\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 0.4572 - accuracy: 0.7950 - val_loss: 0.4677 - val_accuracy: 0.8213\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 44s 351ms/step - loss: 0.4544 - accuracy: 0.8010 - val_loss: 0.2738 - val_accuracy: 0.7937\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 43s 345ms/step - loss: 0.4879 - accuracy: 0.7920 - val_loss: 0.3738 - val_accuracy: 0.8288\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.4838 - accuracy: 0.7840 - val_loss: 0.6237 - val_accuracy: 0.7738\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 39s 315ms/step - loss: 0.4656 - accuracy: 0.7865 - val_loss: 0.4941 - val_accuracy: 0.7725\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 39s 311ms/step - loss: 0.4576 - accuracy: 0.8010 - val_loss: 0.7038 - val_accuracy: 0.7588\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 39s 310ms/step - loss: 0.4596 - accuracy: 0.7960 - val_loss: 0.4791 - val_accuracy: 0.7937\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.4648 - accuracy: 0.7985 - val_loss: 0.6577 - val_accuracy: 0.7912\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 39s 316ms/step - loss: 0.4559 - accuracy: 0.8010 - val_loss: 0.3639 - val_accuracy: 0.7887\n"
     ]
    }
   ],
   "source": [
    "data_Ent = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "gen_entrenamiento = data_Ent.flow_from_directory(\n",
    "    Entrena_Dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batchsize,\n",
    "    class_mode='binary')\n",
    "\n",
    "gen_validacion = test_datagen.flow_from_directory(\n",
    "    Val_Dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batchsize,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    gen_entrenamiento,\n",
    "    steps_per_epoch=Pruebas_ent // batchsize,\n",
    "    epochs=epochs,\n",
    "    validation_data=gen_validacion,\n",
    "    validation_steps=Preubas_Val // batchsize)\n",
    "\n",
    "model.save_weights('Pesos_futuroentrenamiento.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "Podemos observar que contamos con un accuracy para la validación mientras aumentan los epoch del 0.82 para algunos casos y un 0.69 al inicio de la operación del modelo. En general estamos entre el 0.77 - 0.82 para la exactitud del modelo. \n",
    "\n",
    "\n",
    "Podríamos utilizar todas las validaciones y usar un modelo que utilice validación cruzada o bien un auto set de pesos y pipelines para mejorar al 0.9%. Otra estrategia sería utilizar más data augementation y variaciones de pesos con un tuning mayo para aumentar la regularización de nuestro sistema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
